\chapter{Formal Methods of Scheduling Overloaded Real-Time Systems}
\label{fm}
\section{General Approach for Task Scheduling}
The timing behavior of a real-time system is analyzed by modelling all software activities running in the processor as a set of $n$ real-time tasks: $T = \{\tau_1, \tau_2, ..., \tau_n\}$.
A task $\tau_i$ is considered as an infinite sequence of instances, or jobs $\tau_{i,j}$.
A task is said to be periodic if all its jobs are released one after the other with an interval $T_i$ called the task period.
% Each job has a computation time $c_{i,j}$, a release time $r_{i,j}$ and an absolute deadline $d_{i, j}$\cite{lee2007handbook}.
% For simplicity, all jobs of the same task are assumed to have the same computation time and relative deadline, which will be denoted in the following text as $c_i$ and $d_i$, respectively.

The operation of allocating the CPU to a selected task is referred to as \textit{dispatching}.
A set of rules that determines the selection of a task to be executed at current time is called a 
\textit{scheduling algorithm}.
The scope of this work considers scheduling various sets of concurrent periodic tasks which execute on a single processor.
The formal definition of a schedule is stated in the following text.
\begin{mydef}
A schedule is defined as a function $\sigma : \mathbb{R}^{+} \rightarrow \mathbb{N}$ such that 
$\forall t \in \mathbb{R}^{+}, \exists \; t_1, t_2$ such that $t \in [\, t_1, t_2 \rangle$ and 
$\forall \; t' \; \exists \; [\, t_1, t_2 \rangle \; \sigma(t) = \sigma(t')$.
\end{mydef}
In other words, $\sigma(t)$ is an integer step function and $\sigma(t) = k$, with $k > 0$, means that task $J_k$ is is executing at time $t$, while $\sigma(t) = 0$ means that the CPU is idle.
An example of a schedule function is shown in figure \ref{schedule_function}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/sched_function.pdf}
    \caption{Schedule obtained by executing tasks $J_1$, $J_2$ and $J_3$. Modified from \cite{buttazzo2011hard}.}
    \label{schedule_function}
\end{figure}

A schedule is said to be \textit{feasible} if all tasks can be completed according to a set of specific constraints.
A set of tasks is said to be \textit{schedulable} if there exists at least one algorithm that can produce a feasible schedule.

\subsection{Timing Constraints of Real-Time Tasks}
In this work, the timing constraints of the tasks are of interest, specifically the task deadlines.
A deadline represents the time before which a task should complete its execution without causing any damage to the system.
A deadline specified with respect to the task arrival time is called an \textit{absolute deadline}, whereas a deadline specified with resprect to time zero is called a \textit{relative deadline}.
The timing parameters characterizing a real-time task $\tau_i$ are the following:
\begin{itemize}
    \item arrival time $a_i$ - the time at which a task becomes ready for execution,
    \item computation time $c_i$ - the CPU time necessary for executing the task without interruption,
    \item absolute deadline $d_i$ - the time before which a task should be completed to avoid damage to the system,
    \item start time $s_i$ - the time at which a task starts its execution,
    \item finish time $f_i$ - the time at which a task finishes its execution.
\end{itemize}
The timing parameters of a task are illustrated in Fig. \ref{task_timing}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/task_timing.pdf}
    \caption{Timing parameters of a real-time task.}
    \label{task_timing}
\end{figure}
The task lateness represents the delay of a task completion with respect to its deadline:
\begin{align*}
L_i = f_i - d_i.
\end{align*}
If a task completes before the deadline, its lateness is negative.
A parameter which considers task completion after the deadline is the task tardiness - the amount of time a task stays active after its deadline:
\begin{align*}
E_i = max( 0, L_i ).
\end{align*}

Task importance is specified through a value parameter, associated with each task.
Depending on particular application, task value may be directly related to task parameters such as computation time or deadline value, but can also be set to an arbitrary value. 
The value associated with a task as a function of its completion time is called a \textit{utility function}.
Utility function is used to describe the benefit of executing a task depending on the time at which the task is completed in relation to the task deadline.
Depending on the consequences of a missed deadline, real-time tasks are usually distinguished in four categories \cite{buttazzo2011hard}:
\begin{itemize}
    \item non real-time tasks,
    \item soft real-time tasks,
    \item firm real-time tasks,
    \item hard real-time tasks.
\end{itemize}
The typical utility functions differ for the stated categories and are shown in Fig. \ref{utility}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/task_value.pdf}
    \caption{Utility functions for different categories of real-time tasks.}
    \label{utility}
\end{figure}
Non real-time tasks do not have deadlines. The task value is proportional to the task importance and it does not depend on the completion time.
For soft real-time tasks, the task value is constant if the task finishes before its deadline, but it decreases with the exceeding time. 
Thus, a soft deadline is not critical, but missing the deadline causes performance degradation.
In the case of firm real-time tasks, executing a task after its deadline brings no benefit for the system - the utility function is zero after the deadline. However, missing the deadline does not cause any damage to the system.
Hard real-time tasks must complete before their deadline. Missing the deadline may cause catastrophic consequences on the system.

\subsection{Classification of Scheduling Algorithms}
The algorithms for scheduling real-time tasks proposed in literature can be classified by the following criterions \cite{buttazzo2011hard}:
\begin{itemize}
    \item support for preemption,
    \item static / dynamic algorithms,
    \item offline / online usage,
    \item optimal / heuristic.
\end{itemize}
In preemtive algorithms, the running task can be interrupted at any time to assign the processor to another active task of higher priority.
On the other hand, in non-preemptive algorithms a task is executed until completion, without being interrupted. 
All scheduling decisions are taken as the task terminates its execution.
Static algorithms are those in which scheduling decisions are based on fixed parameters, assigned to tasks before their activation.
In dynamic algorithms, the task parameters are assigned dynamically during execution.
A scheduling algorithm is used offline if it is generated before tasks activation. 
The generated algorithm is stored and executed by a dispatcher.
Online scheduling algorithms make scheduling decisions every time a new task enters the system or when a running task terminates.
An algorithm is said to be optimal if it minimizes some given cost function defined over the task set. 
When no cost function is defined, an algorithm is optimal if it is able to find a feasible schedule.
An algorithm is heuristic if it is guided by a heuristic function in taking its scheduling decisions. 
It tends toward the optimal schedule, but it does not guarantee finding it \cite{buttazzo2011hard}.

\subsection{Task Management in Real-Time Systems}
A task competing for CPU time is called an \textit{active} task.
If an active task is currently executing, it is referred to as the \textit{running} task.
The other tasks waiting for CPU time are in the \textit{ready} state and are kept in the \textit{ready queue}.
Fig. \ref{ready_queue} shows a schematic view of the ready queue.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/ready_queue.pdf}
    \caption{Queue of ready tasks waiting for execution. Modified from \cite{buttazzo2011hard}.}
    \label{ready_queue}
\end{figure}
In operating systems that allow dynamic activation, the running task can be interrupted at any moment if a more important task arrives in the system.
CPU is assigned to the most important ready task, while the running task is inserted in the ready queue.
This operation is called \textit{preemption}.
Preemption usually increases the efficiency of the schedule in a sense that it allows executing real-time task sets with higher processor utilization \cite{buttazzo2011hard}.

In standard queueing theory, system load $\rho$ represents the expected number of job arrivals per mean service time. 
In a system consisting of preemptable periodic tasks with implicit deadlines, system load is equivalent to the processor utilization factor:
\begin{equation*}
\rho = U = \sum_{i=1}^{n}\frac{C_i}{T_i},
\end{equation*}
where $C_i$ is the computation time and $T_i$ is the period of task $\tau_i$.
If load value is greater than one, the task set is non-schedulable because the total computation demand requested by the periodic tasks exceeds the available processor capacity.
Hence, not all tasks can complete within their deadlines.
This condition in real-time task scheduling is called an \textit{overload condition}.
System overload may occur due to bad system design, simultaneous arrival of "unexpected" events, malfunctioning of input devices, operating system exceptions and so on.
Methods of handling overload conditions presented in this chapter consider firm real-time tasks.

\section{Transient Overload}
Transient overload occurs for a limited duration, in a system with the average load less or equal to one.
Due to aperiodic requests or unexpected behaviour of some tasks, the maximum load can exceed one and cause a transient overload.

\subsection{Aperiodic Overloads}
This type of overload occurs in systems consisting of aperiodic jobs, due to excessive event arrivals. 
This type of overload can lead to a phenomenon called the \textit{Domino Effect} in which the arrival of a new task can cause all of the previous tasks to miss their deadlines. 
Fig. \ref{aperiodic} depicts such situation on an EDF-scheduled task set. 
The execution of task $\tau_0$ leads to deadline miss of all the previous tasks.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.70\textwidth]{images/overload_aper.pdf}
    \caption{An example of transient overload due to aperiodic job arrival under EDF-scheduled system.}
    \label{aperiodic}
\end{figure}

As shown in \cite{buttazzo2011hard}, it is impossible to create an optimal on-line algorithm that handles overload situations.
Since aperiodic environment requires using an online scheduler, there is no algorithm that can guarantee a feasible schedule.
However, there are different online scheduling algorithms proposed in the literature and they can be divided into three main classes:
\begin{itemize}
    \item{best effort,}
    \item{with acceptance test,}
    \item{robust.}
\end{itemize}

Best effort algorithms always accept new tasks into the ready queue upon arrival. Tasks to be skipped are determined through proper priority assignment with respect to their value.

Algorithms with acceptance test perform a guarantee test at every job activation. 
A new task that enters the system is accepted if the task set is found schedulable by the guarantee routine. Otherwise, task is rejected.
If an acceptance test is performed, the system is able to avoid domino effects.
The disadvantage of acceptance tests is that they do not take task importance into account and always reject newly arrived tasks.

Robust algorithms separate the schedulability verification into two routines: one for task acceptance and one for task rejection. 
Task rejection routine solves the problem of rejection of the newly arrived tasks with high importance.

\subsection{Task Overruns}
In overrun condition, a task (or a job) exceeds its expected utilization value.
Overruns occur if a job computation time exceeds the expected value, or due to early activation of the next job.
Transient overloads due to overruns occur if tasks execute for a longer time than expected, or more frequently than expected.

Fig. \ref{transient_EDF} shows the effect of transient overload condition under EDF-scheduled system. Task $\tau_2$ exceeded its expected computation time, consequently causing task $\tau_1$ to miss its deadline.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/tran_overload.pdf}
    \caption{An example of transient overload due to task overrun under EDF-scheduled system.}
    \label{transient_EDF}
\end{figure}

Overruns are prevented by either aborting the job experiencing an overrun or letting it continue with a lower priority. 
The first solution is not safe because the job could be in a critical section when aborted, thus leaving a shared resource with inconsistent data 
\cite{buttazzo2011hard}. 

A method for implementing the second solution is called resource reservation.
Each task is assigned a fraction of the processor bandwidth, just enough to satisfy its timing constraints, while the kernel prevents each task from consuming more than the requested amount \cite{buttazzo2011hard}. 
A general approach for implementing this method is to reserve each task an amount of processor time $Q_i$ in every time interval $P_i$. 
The reservations can be hard or soft: a hard reservation allows the reserved task to execute at most for $Q_i$ units of time every $P_i$, while a soft reservation allows it to execute for more than $Q_i$ units if there is idle time available. 
In this way, the tasks are effectively reshaped and they can be considered as 
periodic real-time tasks with parameters \( (Q_i, P_i) \) that can be scheduled by a classical real-time scheduler.
The disadvantage of this approach is that the system performance becomes dependent on correct resource reservation. The task may slown down the system if is assinged a much less CPU bandwidth than its average requested value. Contrarily, if the allocated bandwidth is greater than needed, the system will run with low efficiency.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Permanent Overload}

Permanent overload in periodic task systems occurs when the total utilization factor of the periodic task set is greater than one \cite{buttazzo2011hard}. 
This condition occurs either because of wrong estimation of the task execution time, unexpected activation of new periodic tasks, or the increase of activation rate of current tasks.
Methods for reducing permanent overload are the following:
\begin{itemize}
	\item{job skipping,}
	\item{period adaptation,}
	\item{service adaptation.}
\end{itemize}

\subsection{Job Skipping}
\label{skip_algs}
Job skipping method reduces system load by skipping some jobs in the task set. Jobs to be skipped are assigned in a way that the remaining jobs can be scheduled within their deadlines.
This method is applicable for firm real-time tasks, as they allow a certain miss ratio. 
The quantified ratio of tasks that may not be executed is directly related to Quality of Service 
(QoS) metric. 
The algorithms presented in this section have the purpose of maximizing the QoS of periodic tasks.

A task model suitable for job skipping method is known in literature as the firm periodic model.
It was first described in \cite{koren1995skip}.
According to this model, every task is described as following:
\begin{align*}
\tau_i(c_i, T_i, d_i, s_i)
\end{align*}
where $c_i$ is the worst-case computation time, $T_i$ is the task period, $d_i$ is the relative deadline (assumed equal to the period) and $s_i$ is the skip parameter which marks the minimum distance between two consecutive skips, \(2 \leq S_i < \infty\).

An example of schedule with task skipping is given in figure \ref{skipover}. 
Since the utilization factor equals 
\begin{equation*}
U_p = \sum_{i=1}^{n} \frac{C_i}{T_i} = 1.17,
\end{equation*}
the system is permanently overloaded. 
However, if task \(\tau_2\) is skipped every three instances, the overload is resolved.
\\
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/skipover.pdf}
    \caption{An example of resolving permanent overload by job skipping method.}
    \label{skipover}
\end{figure}

Corresponding to the stated task model, every job of a periodic task can be red or blue.
A red job must be completed within its deadline, whereas a blue job can be aborted at any time 
\cite{buttazzo2011hard}. 
With respect to the skip parameter $S_i$, each scheduling algorithm needs to fulfill two conditions:
\begin{itemize}
	\item if a blue job is skipped, then the next \(S_i - 1\) jobs must be red,
	\item if a blue job completes successfully, the next job is also blue.
\end{itemize}
The authors of the model proved that the problem of determining whether a set of periodic skippable tasks is schedulable is NP-hard \cite{koren1995skip}.

For the given set of skippable periodic tasks \( \Gamma = {\tau_i(C_i,T_i,S_i)} \), the necessary schedulability condition is the following:
\begin{equation}
\sum_{i=1}^{n} \frac{C_i(S_i-1)}{T_i S_i} \leq 1.
\end{equation}

A sufficient condition for guaranteeing schedulability of a set of skippable tasks can be stated 
using the equivalent utilization factor, which is formally described in the following definition:

\begin{mydef}
Given a set \( \Gamma = {\tau_i(C_i,T_i,S_i)} \) of n skippable periodic tasks, the equivalent utilization factor in time interval \([0,L]\) is defined as:
\begin{equation*}
U_p^\ast = \max_{L \geq 0} \frac{\sum_{i}D(i, [0,L])}{L}
\end{equation*}
where
\begin{equation*}
D(i, [0,L]) = \left\lfloor \frac{L}{T_i} - \frac{L}{T_i \cdot S_i}\right\rfloor \cdot C_i.
\end{equation*}
\end{mydef}

According to the sufficient schedulability condition, a set of skippable periodic tasks is schedulable if \( U_p^\ast \leq 1 \).

\subsubsection{RTO Algorithm}
The Red Tasks Only (RTO) algorithm decreases system load by always skipping the blue tasks, while red tasks are scheduled according to EDF. 
% The algorithm becomes optimal under a certain condition, the \textit{deeply-red} condition:
% all tasks are synchronously activated and the first \(S_i - 1\) instances of every task $\tau_i$ are red. 
% The stated condition is the worst-case condition for periodic task sets.
As all instances of blue tasks are systematically rejected rregardless of their impact on the equivalent utilization factor, RTO schedule has the lowest QoS level for given task set.
Implementation of RTO algorithm was tested on an example shown in 
figure \ref{rto}.
Total processor utilization $U_p$ equals 1.19 and the equivalent processor 
utilization $U_p^\ast$ equals 0.79.
As the schedule results in seven missed instances, QoS equals 0.53. 
\\
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/skipover_RTO.pdf}
    \caption{An example of a schedule produced by RTO algorithm with skipping factor $s_i=2$.}
    \label{rto}
\end{figure}

\subsubsection{BWP Algorithm}
Blue When Possible (BWP) algorithm introduces amendments of RTO algorithm disadvantages in a way that blue tasks execute only if there are no red ready instances.
As BWP increases the number of task instances that complete successfully, it offers a higher QoS level. 
Fig. \ref{bwp} depicts the same overload condition as described in figure 
\ref{rto}, but solved by BWP algorithm. 
Total number of completed task instances is increased because two blue instances completed successfully, thus increasing QoS to 0.67.
\\
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/skipover_BWP.pdf}
    \caption{An example of a schedule produced by BWP algorithm with skipping factor $s_i=2$.}
    \label{bwp}
\end{figure}

\subsubsection{RLP Algorithm}
The Red Tasks as Late as Possible (RLP) algorithm stimulates the execution of blue tasks by executing them at idle times of a schedule considering only the red tasks. 
A basis of this approach is the Earliest Deadline as Late as Possible (EDL) algorithm. 
EDL is used to build a schedule on the red instances only, where red instances execute as late as possible. 
In the remaining EDL idle times, blue tasks are executed as soon as possible. 
The main idea of EDL is to maximize the length of idle time periods at the beginning of the schedule. 

Determination  of  the  duration  and  position  of  these  idle  times  is  done  by mapping out the EDL schedule produced from time zero up to the end of the first hyperperiod \cite{ghor2011real}.
This approach makes any spare processing time available as soon as possible, 
thus effectively stealing slack from the hard deadline periodic tasks 
\cite{queudet2012quality}. 
Available slack at any time can be determined by mapping out an EDL schedule from current time to the end of the current hyperperiod.
EDL schedule is constructed dynamically at run-time from a static EDL schedule.
A static EDL schedule is constructed off-line and determined by two vectors:
\begin{itemize}
	\item{$K$ - static deadline vector,}
	\item{$D$ - static idle time vector.}
\end{itemize}
Vector $K$ stores the time instants from 0 to the end of the first hyperperiod at which idle times occur. 
Vector $D$ represents the lengths of idle times starting at time instances stored in $K$. 
Dynamic EDL schedule takes the execution of current running task into account and it is described by the following vectors:
\begin{itemize}
	\item{$K_t$} - dynamic deadline vector,
	\item{$D_t$} - dynamic idle time vector.
\end{itemize}
$K_t$ represents the instants $k_i$ from $t$ in the current hyperperiod at which idle times occur. 
The lengths of idle times starting at instants $k_i$ are stored in the vector $D_t$. 

% As EDL idle time is determined by the described vectors, EDL schedule itself is not considered in this work because the vectors can be computed with the aid of EDF schedule.

In this work, EDL idle times are computed through corresponding EDF values, according to the expression stated in \cite{chetto1989some}:
\begin{align}
f_T^{EDL}(H - t) = f_t^{EDF}(t),
\end{align}
Where $H$ is the hyperperiod and $f_t$ is the availability function:
\begin{equation*}
f_T(t) = \begin{cases}
1 &\text{if processor is idle at $t$,}\\
0 &\text{else.}
\end{cases}
\end{equation*}

Consequently, EDL static deadline vector can be computed from EDF vector by simply reversing the elements.

EDL deadline vector values can be calculated according to the following formula:
\begin{align}
k_i^{EDL} = H - k_i^{EDF} - c_i.
\end{align} 

The idle function for given time instance is updated during the execution of RLP algorithm. 
Update of EDL algorithm takes into account the current red ready tasks and the current running task, if its current state is red. 
Dynamic EDL algorithm is calculated whenever a blue task is released, or a blue task is completed. 

% If there are no blue instances ready for execution, red instances are scheduled according to EDF algorithm.
% Otherwise, red instances are processed as late as possible by the EDL rule, while blue instances are executed as soon as possible in the remaining EDL idle times.
A pseudo-code for RLP algorithm is given in Alg. \ref{alg:rlp}.
The tasks are stored in three lists, sorted in increasing order of deadline: 
\begin{itemize}
	\item{waiting list: list of instances waiting for their next release,}
	\item{red ready list: list of red instances in ready state,}
	\item{blue ready list: list of blue instances in ready state.}
\end{itemize}
At every time slice, the scheduler checks whether any instances changed their state from ready to waiting or vice versa and updates all three lists. 
If there is a missed instance of a task in blue ready list, task is aborted and its new instance is put into waiting list. 
Next, the waiting list is checked in order to indentify ready tasks. 
Ready tasks are put into red and blue ready list according to their state and the idle function value at current time. 
If current time belongs to an EDL idle time, the first instance in the blue ready list is selected for execution. Otherwise, the first instance from the red ready list is selected.

\begin{algorithm} % TODO staviti \small i captione
\caption{RLP scheduling algorithm.\label{alg:rlp}}
\begin{algorithmic}[1]
\State initialize tasks
\For{task in blue\_ready}
\If{$task_{next\_instance}$}
\State abort task
\EndIf
\EndFor
\For{task in waiting\_list}
\If{\! $task_{ready}$}
\State break
\EndIf
\If{$task_{state}$==RED
$\And f_{EDL}(t)=0$}
\State pull task from $waiting\_list$
\State put task into $red\_ready$ list
\Else
\If{$blue\_ready = \emptyset$}
\State compute EDL schedule
\EndIf
\If{$f\_EDL(current\_time) \neq 0$}
\State pull task from $waiting\_list$
\State put task into $blue\_ready$ list
\EndIf
\EndIf
\EndFor
\If{$blue\_ready \neq \emptyset$ 
$\And f\_EDL(current\_time) \neq 0$}
\For{task in red\_ready list}
\State pull task from red ready list
\State put task into ready list
\EndFor
\EndIf
\end{algorithmic}
\end{algorithm}

An implementation of the RLP algorithm was tested on the same set of periodic tasks as in the previous sections and the resulting schedule is shown in Fig. 
\ref{rlp_schedule}.
Since there are four deadline misses, the QoS is increased to 0.73.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/skipover_RLP.pdf}
    \caption{An example of a schedule produced by RLP algorithm with skipping factor $s_i=2$.}
    \label{rlp_schedule}
\end{figure}

A comparison of QoS value depending on the utilization value for the described algorithms is shown in Fig. \ref{fm_comparison}.
The scattered plot shows the results of testing each algorithm on $1500$ different task sets with utilization factors from the interval $[0.90, 1.60]$.
An eighth degree polynomial regression model for each algorithm is shown by full line.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/qos_comp_polyfit.pdf}
    \caption{Comparison of QoS value for RTO, BWP and RLP algorithm.}
    \label{fm_comparison}
\end{figure}

% \begin{algorithm}
% \caption{RLP scheduling algorithm.\label{alg:rlp}}
% \begin{algorithmic}[1]
% \State initialize tasks 

% \If{$running\_task$} 
% \If{$current\_time == running\_task_{due\_date}$ 
% \And $running\_task_{remaining\_time} > 0$}
% \State $running\_task_{state} = RED$ \COMMENT{running task is going to miss deadline}
% \State $set\_next\_instance( running\_task )$
% \State $waiting\_list \leftarrow running\_task$
% \EndIf
% \EndIf


% \While{$blue\_ready\_list \neq \emptyset$}

% \State $task \leftarrow next(blue\_ready\_list)$
% \If{$task_{release\_time} + task_{critical\_delay} < current\_time$} 
% \State break
% \EndIf
% \State $task \leftarrow pull(blue\_ready\_list)$ \COMMENT{abort blue tasks}
% \State $set\_next\_instance( task )$
% \State $waiting\_list \leftarrow append(task, waiting\_list)$
% \EndWhile
% \COMMENT{checking waiting list in order to release tasks}
% \While{$waiting\_list \neq \emptyset$}
% \State $task \leftarrow pull(waiting\_list)$
% \If{$task_{release_time} > current\_time$}
% \State $break$
% \EndIf 
% \If{$task_{state}==RED$
% \And$f_{EDL}(t)=0$}
% \State$task \leftarrow pull(waiting\_list)$ \COMMENT{red task release}
% \State$red\_ready\_list \leftarrow append(task, red\_ready\_list)$
% \Else
% \If{$blue\_ready\_list \neq \emptyset$}
% \State$compute$ $EDL\_schedule$
% \EndIf
% \If{$f\_EDL(current\_time) \neq 0$}
% \State $task \leftarrow pull(waiting\_list)$
% \State $blue\_ready\_list \leftarrow append(task, red\_ready\_list)$
% \COMMENT{blue task release}
% \EndIf
% \EndIf
% \State $task_{current\_skip\_value} += 1$
% \EndWhile
% \If{$blue\_ready\_list \neq \emptyset$ 
% \And $f\_EDL(current\_time) \neq 0$}
% \While{$task \leftarrow next(red\_ready\_list)$}
% \State $task \leftarrow pull(red\_ready\_list)$ \COMMENT{suspend red task}
% \State $waiting\_list \leftarrow append(task, waiting\_list)$
% \EndWhile
% \EndIf
% \end{algorithmic}
% \end{algorithm}

\newpage
\subsection{Period Adaptation}
If a periodic task set experiences a permanent overload, one way of reducing the system load is to vary tasks' rates in order to keep the total load under below a desired threshold.
A method of reducing the system load by enlarging task periods is called period adaptation. 
Whenever the system cannot guarantee the acceptance of a new task, instead of rejecting it, the system can try to reduce the utilizations of other tasks by increasing their periods. In that way, the total load is decreased and the new task may be accepted.

A method for changing task periods as a function of the desired workload is the elastic framework. Each task is considered as flexible as a spring, whose utilization can be modified by changing its period within a specified range 
\cite{lee2007handbook}.

Each task $\tau_i$ is detoned by:
\begin{equation*}
\tau_i(C_i, T_{i_{min}}, T_{i_{max}}, E_i),
\end{equation*}
where $C_i$ is computation time, $T_{i_{min}}$ and $T_{i_{max}}$ are its minimum and maximum period, respectively.
Minimum period is considered as a nominal period. 
 $E_i$ is the elastic coefficient which specifies the flexibility of the task.
Corresponding to $T_{i_{min}}$ and $T_{i_{max}}$, the maximum and minimum utilization of the task set is defined in the following way:
\begin{align*}
U_{max} = \sum_{i=1}^{n}\frac{C_i}{T_{min}},\\
U_{min} = \sum_{i=1}^{n}\frac{C_i}{T_{max}}.
\end{align*}

For example, in a task set scheduled by EDF, the elastic model can be used to adapt the task periods so that the task set becomes schedulable:
\begin{equation*}
\sum_{i=1}^{n}\frac{C_i}{T_i} = U_d \leq 1,
\end{equation*}
where $U_d$ is the desired utilization factor.
This is possible as long as the desired utilization value is grater or equal to minimum utilization value.

According to \cite{lee2007handbook}, if there are no constrains on the maximum tesk periods, the utilization of each compressed task can be computed by the following expression:
\begin{equation*}
U_i = U_{i_{max}} - ( U_{max} - U_d ) \cdot \frac{E_i}{E_{tot}},
\end{equation*}
where \( E_{tot} = \sum_{i=1}^{n}E_i \).

The main advantage of this approach is that a new period configuration can be determined online based on the values of elastic coefficients, which are set beforehand based on some design criterion.
On the other hand, in the presence of period constraints, the compression algorithm becomes iterative with complexity \( O(n^2) \) \cite{lee2007handbook}.

\subsection{Service Adaptation}
Service adaptation method reduces the system load by decreasing the task computation times. 
This approach is only applicable with the tasks whose result quality is directly related to computation time.
An example of such tasks are algortihms that produce approximated results where the precision of the results depends on the number of iterations, and thus with the computation time. 
An overload condition can be handled by reducing the quality of results, aborting the remaining computation if the quality of the current result is acceptable \cite{shih1989scheduling}. 
The described concept is known in literature as imprecise computation. 



% In a system that supports imprecise computation, every task is divided into a mandatory subtask $M_i$ and an optional subtask $O_i$. 
% Mandatory subtask must be completed in order to produce a result of acceptable quality, whereas the optional subtask refines this result 
% \cite{shih1989scheduling}. 
% Both subtasks have the same arrival time and deadline as the original task, but the optional subtask is ready for execution after the mandatory task is completed. 

