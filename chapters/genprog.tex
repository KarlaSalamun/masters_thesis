\chapter{Genetic Programming}
\label{genprog}
\section{Optimization}
An optimization problem is the problem of finding the best solution to some problem instance. % from all feasible solutions.
Depending on the number of objective functions, optimization problems are distinguished in two categories: single-objective and multi-objective optimization problems.

\subsection{Single-objective optimization}
A single-objective optimization problem in its general form is defined in the following way:
\begin{align*}
\label{opt_problem}
\text{minimize/maximize } & f\_(\textbf{x}),   \\
\text{subject to } & g_j(\textbf{x}) \geq 0, & j = & 1, 2, ..., J \\
           & h_k(\textbf{x}) = 0, & k = & 1, 2, ..., K \\
           & x_{i}^{L} \leq x_i \leq x_{i}^{U}, & i = & 1, 2, ..., n. 
\end{align*}
Function $f(\textbf{x})$ is the objective function for the problem.
A solution $\textbf{x}$ is a vector of $n$ decision variables: $\textbf{x} = \{x_1,x_2,...,x_n\}$.
The set of $J$ functions $g_j(\textbf{x})$ is called inequality constraints, while the set of $K$ functions $h_k(\textbf{x}$ is reffered to as equality constraints.
In order for a solution to be feasible, all constraints, specifed with constraint functions and their respective bounds, have to be satisfied.
The last set of constraints defines a lower $x_{i}^{L}$ and an upper $x_{i}^{U}$ bound for the values that each decision variable. 
The described sets of constraints define a decision space $D$.
Each solution \textbf{x} is a single point in the decision space.
% The objective function $f$ to be maximized or minimized is often reffered to as the \textit{fitness} function.
Maximization problems can be reduced to minimization problems and vice versa by translating the value of the objective function to its complementary value, $-f$.
Thus, the greater value of a fitness function corresponds to a better solution.

\begin{mydef}
A solution $\textbf{x}^{*}$ is a global optimum if the solution $\textbf{x}^{*}$ is feasible and if $f(\textbf{x}^{*}) \geq f(\textbf{x})$ for every other solution $\textbf{x}$ from the feasible solution space.
\end{mydef}
If the function $f$ is not strictly convex, the existance of one or more local optima is possible.
\begin{mydef}
Solution $\textbf{x}^{*}$ is a local optimum if and only if the solution $\textbf{x}^{*}$ belongs to the feasible solution space and if for every other solution $\textbf{x}$ such that 
$\delta > 0, |\textbf{x} - \textbf{x}^{*}| \leq \delta$, the condition $f(\textbf{x}^{*}) \geq f(\textbf{x})$ is fulfilled.
\end{mydef}

If an optimization algorithm deterministically chooses the better solution than the current solution, the solutions generally converge towards the local optimum and the algorithm yields the wrong result.
A possible solution to this problem is stochastically accepting the solutions that are worse than the current solution.

\subsection{Multi-Objective Optimization}
A multi-objective optimization problem consists of multiple objective functions which are maximised or minimised. 
The general form of a multi-objective optimization problem is stated as follows:
\begin{align*}
\text{minimize/maximize } & f_m(\textbf{x}),  & m = & 1, 2, ..., M \\
\text{subject to } & g_j(\textbf{x}) \geq 0, & j = & 1, 2, ..., J \\
           & h_k(\textbf{x}) = 0, & k = & 1, 2, ..., K \\
           & x_{i}^{L} \leq x_i \leq x_{i}^{U}, & i = & 1, 2, ..., n. 
\end{align*}

While optimizing a set of $M$ functions, the problem can be stated in a way that some functions need to be maximized, while the others need to be minimized. 
Since the minimization problem of a function $f_i$ can be reduced to maximization of the function $-f_i$, the general form can be stated as a problem of maximization or minimization of all $M$ functions.

In multi-objective optimization, the objective functions constitute a multi-dimensional space $Z$, called the objective space. 
For each solution $\textbf{x}$ in the decision variable space, there exists a point in the objective space, denoted by $f(\textbf{x}) = \textbf{z} = \{z_1, z_2, ... z_M\}$ 
\cite{deb2001multi}. 
Since $\textbf{z}$ is a vector, two solutions 
$\textbf{x}^{(1)}$ and $\textbf{x}^{(2)}$ with corresponding vectors $\textbf{z}^{(1)}$ and $\textbf{z}^{(2)}$ cannot be directly compared. 

In most multi-objective optimization algorithms, comparison between solutions is done with the aid of the domination concept. 

\begin{mydef}
A solution $\textbf{x}^{(1)}$ is said to dominate the other solution $\textbf{x}^{(2)}$, if both conditions 1 and 2 are true: 
\begin{enumerate}
    \item The solution $\textbf{x}^{(1)}$ is no worse than $\textbf{x}^{(2)}$ in all objectives, or $z_j^{(1)} \geq z_j^{(2)}, \forall j \in {1,...,M}$.
    \item The solution $\textbf{x}^{(1)}$ is strictly better than $\textbf{x}^{(2)}$ in at least one objective, or $\exists j \in {1,...,M}: z_j^{(1)} > z_j^{(2)}$.
\end{enumerate}
\end{mydef}
There are three possibilities that can be the outcome of the dominance check between two solutions:
\begin{itemize}
    \item solution 1 dominates solution 2, 
    \item solution 1 gets dominated by solution 2,
    \item solutions 1 and 2 do not dominate each other.
\end{itemize}

In optimization algorithms, dominance is used to determine the quality of the solutions. 
By performing the dominance check between every pair of solutions, the solution set can be divided into two subsets: the solutions that are not dominated by other solutions and the solutions that get dominated by at least one solution. 
A set of all feasible solutions that are not dominated by any other solution is called a globally Pareto-optimal set. 

Some multi-objective optimization algortihms require sorting the population in different subsets with respect to their closeness to the non-dominated subset \cite{cupic2013prirodom}. 
Such subsets are referred to as fronts. 

A method for sorting the population into fronts is called non-dominated sorting. 
The complexity of the basic approach for non-dominated sorting is $\mathcal{O}(MN^3)$, where $M$ is the number of objective functions and $N$ is the population size.
An approach for non-dominated sorting used in this work is described with Alg. 
\ref{nondomsort}. This is a faster and more acceptable method for dominated sorting which requires at most $\mathcal{O}(MN^2)$ computations \cite{deb2001multi}.
For every solution $i$, a domination count $\mu_i$ is calculated. 
The domination count is the number of solutions which dominate the solution $i$. 
The solutions that have $\mu_i = 0$ form a non-dominated set.
Secondly, for every solution a set $S_i$ is determined. The set $S_i$ consists of the indices of all of the solutions that are dominated by the solution $i$. 
The solutions with $\mu_i=0$ are set as the first front. 
The next part of the process is to compute subsequent fronts.
This is done by finding the dominated solutions in the remaining population for each solution in the last computed front.
The counters $\mu_i$ for the dominated solutions are reduced. 
If for any solution the counter $\mu_i$ becomes zero, it means that the solution is not dominated by any solutions in the remaining population and it is added to a new front. 

\newpage
\begin{algorithm}
\caption{Non-dominated sorting.\label{nondomsort}}
\begin{algorithmic}[1]
\ForAll{$i \in P$}
\State set $\mu_i = 0$ and $S_i = \emptyset$
\EndFor
\ForAll{$j \neq i$ and $j \in P$}
\If{solution $i$ dominates solution $j$}
\State $S_i = S_i + j$
\ElsIf{solution $j$ dominates solution $i$}
\State increment counter $\mu_i = \mu_i + 1$
\EndIf
\If{$\mu_i = 0$} 
\State keep solution $i$ in the first non-dominated front $P_i$
\State set front counter $k=1$
\EndIf
\EndFor
\While{$P_k \neq \emptyset$}
\State initialize $Q = \emptyset$ which keeps the next non-dominated front
\ForAll{$i \in P_k \And j \in S_i$}
\State $\mu_j = \mu_j - 1$
\If{ $\mu_j = 0$} 
\State add solution $j$ to current front: $Q = Q + j$
\EndIf
\EndFor
\State set $k = k+1$
\State set $P_k = Q$
\EndWhile
\end{algorithmic}
\end{algorithm}

\section{Genetic Algorithms}

The algorithms that search the solution space to find good solutions, but do not guarantee finding the optimal solution are called \textit{heuristics}. 
Heuristics typically have relatively low computational complexity \cite{cupic2013prirodom}.
A \textit{metaheuristic} can be defined as an upper level general methodology (template) that can be used as a guiding strategy in designing underlying heuristics to solve specific optimization problems \cite{talbi2009metaheuristics}.

Stohastic optimization algorithms inspired by processes in biological evolution are called \textit{evolutionary algorithms}.
Besides stochastic optimization, evolutionary algorithms have a metaheuristic character.
In this context, a solution to a given optimization problem is called an \textit{individual} and a set of solutions is called a \textit{population}.
The basic approach in all evolutionary algorithms is applying evolutionary operators (selection, crossover, mutation and recombination) to a population of solutions.
Every interation of the algorithm corresponds to a generation.
As a result, the population gradually evolves towards better areas of the search space. 
The value which indicates how well the solution fulfills the problem objective is referred to as \textit{fitness} value.

Evolutionary algorithms are divided into four classes: 
\begin{itemize}
    \item genetic algorithms,
    \item genetic programming,
    \item evolutionary strategies,
    \item evolutionary programming.
\end{itemize}

The general approach in genetic algorithms is to generate a population of genotypes and apply evolution operators on them.
A genotype is a data structure which represents a potential solution to the given problem.

Every genotype is assigned a fitness value by computing the value of the objective function in the point represented by the genotype.
This process is called evaluation.
In every generation, some genotypes are selected from the population by a defined selection operator which favors the best solutions against the worst ones with respect to the fitness function.
The selected members are referred to as parents.
The crossover operator is applied to the parents, creating the children genotypes.
In that way, the recombination of genes is emulated - two or more parts from parent genotypes are combined to form an offspring.
After crossover, the mutation operator is applied to the offspring.
Mutation randomly modifies a single solution.
The algorithm cycle is finished by adding the children to the population by reinsertion operator.

There are two possible implementations of the genetic algorithm: the steady-state genetic algorithm and the generational genetic algorithm.

A schematic view of the steady-state implementation is shown in Fig. \ref{genalg:impl_1}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/genalg_impl_1.pdf}
    \caption{A schematic view of the steady-state genetic algorithm.}
    \label{genalg:impl_1}
\end{figure}
In every generation, two parents are selected.
The offspring is created by performing crossover on the parents and mutating the resulting genotype.
Before inserting into the population, the offspring is evaluated to determine whether it can replace an existing population member.
If the offspring is inserted in the population, it usually replaces the worst genotype in the population.
Alternatively, the genotype to be replaced can be the oldest member, a randomly selected member or it can be determined by a selection operator modified in a way that it prefers selecting worse genotypes.
A pseudocode for the steady-state genetic algorithm is given in Alg. \ref{genalg_ss}.
\begin{algorithm}
\caption{Steady-state genetic algorithm.\label{genalg_ss}}
\begin{algorithmic}[1]
\State $\text{population} \gets create\_initial\_population()$
\State $evaluate(\text{population})$
\While{$! \; stop\_condition$}
\State $\text{parents} \leftarrow selection(\text{population})$
\State $\text{offspring} \leftarrow crossover(\text{parents})$
\State $mutate(\text{offspring})$
\State $evaluate(\text{offspring})$
\State select a genotype to be replaced by offspring
\State decide whether replacement will be performed, replace members if needed
\EndWhile
\end{algorithmic}
\end{algorithm}

In the generational genetic algorithm, in every generation a new offspring population is created, which completely replaces the parent population.
This implementation is shown in Fig. \ref{genalg:impl_2}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/genalg_impl_2.pdf}
    \caption{A schematic view of the generational genetic algorithm.}
    \label{genalg:impl_2}
\end{figure}
In every step of the algorithm, a new temporary population is created, consisting only of the offspring members.
When the size of the new population reaches the size of the parent population, the latter is replaced by the new population.
The basic approach to generational genetic algorithm is not elitistic, i.e. it does not necessarily keep the best solution found. 
A modification of the algorithm that involves elitism directly adds the current best solution (or a few of them) to the offspring population.
The rest of the offspring population is generated by applying crossover on the parents and mutating the result.
An elitistic implementation of the generational genetic algorithm is given in Alg.
 \ref{genalg_g}.

\section{Tree-Based Genetic Programming}
Genetic programming is an extension of the conventional genetic algorithm where the genotypes represent a program that solves the considered problem.
This is achieved by increasing the complexity of the structures undergoing adaptation.
A typical data structure used in genetic programming is a tree which represents a hierarchical computer program of dynamically varying size and shape \cite{koza1992genetic}.
Fig. \ref{genprog:ex1} depicts an example of such data structure which can be represented as a function:
\begin{equation*}
f(x, y, z) = 2 \cdot (x-5) + \frac{y}{z + 3} \, .
\end{equation*}

\begin{algorithm}
\caption{Elitistic variant of the generational genetic algorithm.\label{genalg_g}}
\begin{algorithmic}[1]
\State $\text{population} \gets create\_initial\_population()$
\State $evaluate(\text{population})$
\While{$! \; stop\_condition$}
\State $\text{new\_population} = \emptyset$
\State $\text{new\_population} += get\_best(\text{population})$
\While{$\text{new\_population.size()} < \text{population.size()}$}
\State $\text{parents} \gets selection(\text{population})$
\State $\text{offspring} \leftarrow crossover(\text{parents})$
\State $mutate(\text{offspring})$
\State $\text{new\_population} += \text{offspring}$
\EndWhile
\State $\text{population} \leftarrow \text{new\_population}$
\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{images/gp_ex1.pdf}
    \caption{An example of a tree structure representing a program.}
    \label{genprog:ex1}
\end{figure}

Each genotype is composed of function and terminal nodes.
Function nodes may be standard arithmetic operations, programming operations, mathematical or logical functions.
In Fig. \ref{genprog:ex1}, function nodes are arithmetical operators ($+, -, *, /$), while the other nodes are terminal.
Each terminal node represents a parameter related to the specific problem and can be of various data types.
Moreover, a terminal node can be a constant ($2, 5, 3$ in Fig. \ref{genprog:ex1}) or a variable ($x, y, z$).
As in general genetic algorithms, each individual is assigned a fitness value in the evaluation process.
The computation of the fitness value depends on the nature of the problem.
In general, fitness value measures how well the genotype solves the given problem.

The initial population consists of random compositions of the function and terminal nodes.
The evolution is performed in the following way \cite{koza1992genetic}.
Each program (genotype) in the population is executed and is assigned a fitness value according to how well it solves the problem.
New population is generated by copying a part of the existing genotypes to the new population, while the rest is created by genetically recombining randomly chosen parts of parent genotypes.
The result of genetic programming is the best individual that appeared in any generation.

There are various selection mechanisms proposed in the literature, with different amount of selection pressure (or \textit{greediness}).
As greedy algorithms consider selecting only the best individuals, they have the general property of rapid convergence, but not necessarily to the best solution.
For some problem domains, convergence to local optima is acceptable, but other applications require a slower and more diffuse search \cite{rozenberg2012handbook}.
The most common selection methods include the following, in the order of decreasing selection pressure:
\begin{itemize}
	\item truncation selection: choose only the N best individuals,
	\item tournament selection: choose K individuals at random and keep only the best of the K selected,
	\item rank-proportional selection: choose individuals proportional to their fitness rank in the population,
	\item fitness-proportional selection: choose individuals proportional to the value of their fitness.
\end{itemize}
In this work, the tournament selection is used.

Crossover operator is performed as replacing one parent's subree with a randomly chosen subtree of the other parent.
This is illustrated in Fig. \ref{genprog:ex2}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{images/crossover.pdf}
    \caption{Crossover operator in genetic programming.}
    \label{genprog:ex2}
\end{figure}

Mutation of the offspring can be performed in multiple ways: by replacing a node by a randomly generated subtree, erasing a subtree or a node, rotation of the child nodes etc.
An example of a mutated individual is shown in Fig. \ref{genprog:ex3}.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/mutation_1.pdf}
    \caption{Mutation operator in genetic programming.}
    \label{genprog:ex3}
\end{figure}

\section{Multi-objective Optimization using Genetic Algorithms}
Genetic algorithm based on the concept of non-dominated sorting is the Non-Dominated Sorting Genetic Algorithm (NSGA). 
In the evaluation of the solutions, each individual is assigned a scalar value which represents its fitness. 
The individuals that belong to a front closer to the non-dominated set are assigned a greater fitness value. Additionally, the individuals of the same front with clustered objective values are assigned a diminished fitness value. 
In that way, the diversity between the individuals is stimulated. 

In this work, the algorithm NSGA-II is used for handling multi-objective optimization problems. 
NSGA-II is an improved version of NSGA algorithm and it will be described in the following subsection. 

\subsection{NSGA-II Algorithm}
One of the main disadvantages of the original NSGA algorithm is the lack of elitism which significantly slows down the performance of the genetic algorithm due to the loss of good solutions once they have been found \cite{deb2000fast}. 
The NSGA-II algorithm is a modification of the original NSGA algorithm and its main feature is elitist approach in selecting new population members. 
An implementation of the NSGA-II algorithm is described with Alg. \ref{nsga-ii}.

The NSGA-II algorithm creates an offspring population $Q_t$ of the same size $N$ as the parent population $P_t$.
A combined population $R_t = P_t \cup Q_t$ of size $2N$ is formed.
The population $R_t$ is then sorted according to non-dominated sorting. 
After non-dominated sorting, the population is fragmented into $\rho$ fronts: $\mathcal{F}_1, ..., \mathcal{F}_\rho$.
The new parent population $P_{t+1}$ is formed by adding solutions from the fronts.
Since the size of the new population must be equal to $N$, the fronts are added to the new population in increasing rank order, as long as they can be fully added.

An additional benefit of NSGA-II is the perservation of the diversity between individuals. 
Therefore, the inidividuals that are added to the new population from the last remaining front are determined according to their diversity. 
For finding the most "scattered" solutions, the crowding-sort method is used. 
The density of the solutions surrounding a particular point in the population is estimated by crowding distance. 
In the case of a two-objective problem, this value refers to the average distance of the two points on either side of the particular point along each of the objectives, as shown in Fig. 
\ref{crowding_dist}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.50\textwidth]{images/crowd_dist.pdf}
    \caption{Crowding distance. The crowding distance of the $i$-th solution in its front (marked with solid circles) is the average side-length of the cuboid (marked with dashed lines). }
    \label{crowding_dist}
\end{figure}

Crowding distance $i_{distance}$ is an estimate of the size of the largest cuboid enclosing the point $i$ without including any other point in the population. 
The crowding-sort method arranges the individuals according to the crowding distance value.

After creating the new parent population, an offspring population is created by crowding tournament selection, crossover and mutation. 
Crowding tournament selection is based on the individual's rank (the number of the associated front) and the crowding distance. 
According to the crowding tournament selection, solution $i$ is the tournament winner if any of the following conditions are fulfilled:
\begin{enumerate}
	\item solution $i$ has a better rank than solution $j$: $r_i < r_j$,
	\item solution $i$ has an equal rank as solution $j$, but has a larger crowding distance, i.e. $r_i=r_j, d_i > d_j$.
\end{enumerate}

\begin{algorithm}
\caption{NSGA-II algorithm.\label{nsga-ii}}
\begin{algorithmic}[1]
\State combine the current parent population $P_t$ and offspring population $Q_t$ into population $R_t$, $R_t = P_t \cup Q_t$
\State perform non-dominated sorting of the population $R_t$, which creates $\rho$ fronts: 
$\mathcal{F}_1, ..., \mathcal{F}_\rho$ 
\State set the new parent population to an empty set: $P_{t+1} = \emptyset$
\State set $i=1$
\While{front $i$ can fit completely into population $P_{t+1}$}
\State add current front to $P_{t+1}$: $P_{t+1} = P_{t+1} \cup \mathcal{F}_i$
\State $i=i+1$
\EndWhile
\State perform grouping sort of the front $\mathcal{F}_i$
\State add $N-|P_{t+1}|$ solutions from the front $\mathcal{F}_i$ to $P_{t+1}$
\State perform crowded tournament selection of the population $P_{t+1}$
\State create new offspring population $Q_{t+1}$ by crossover and mutation
\end{algorithmic}
\end{algorithm}

\section{Cooperative Coevolution}
Coevolutionary architectures of genetic algorithms define the fitness evaluation of an individual in relation and adaptation with respect to the other individuals in the population. 
Coevolution can be cooperative or competitive. 
In competitive coevolution, the evaluation of an individual is determined by a set of competitions between itself and other individuals. 
Contrarily, in cooperative coevoluiton, collaborations between a set of individuals are necessary in order to evaluate one complete solution \cite{stoean2014support}. 
Since cooperative coevolution is of interest in this work, this model will be described. 

In cooperative coevolution, two or more genetically isolated species cooperate in solving a target problem. 
Genetic isolation is enforced by evolving the species in separate populations. 
The species exchange information only when the individuals are evaluated. 
Target problem statement is decomposed into components and each subproblem is assigned to a species. 
The fitness of each member is evaluated by forming collaborations with individuals from other species. 

An application of cooperative coevolution is optimization of functions with multiple variables. 
Each variable is considered as a component of the solution vector and corresponds to a single population. 

The general cooperative coevolutionary algorithm is described with Alg. \ref{coop}.

\begin{algorithm}
\caption{Cooperative coevolution algorithm.\label{coop}}
\begin{algorithmic}[1]
\State set $t=0$
\ForAll{species $s$}
\State randomly initialize population $P_s(t)$
\EndFor
\ForAll{species $s$}
\State evaluate $P_s(t)$ by choosing collaborators from the other species
\EndFor
\While{termination condition is not satisfied}
\ForAll{species $s$}
\State select parents from $P_s(t)$
\State apply crossover and mutation
\State evaluate offspring by choosing collaborators from the other species
\State select new population $P_s(t+1)$
\EndFor
\State set $t=t+1$
\EndWhile
\end{algorithmic}
\end{algorithm}

First, each population is initialized.
In the first evaluation, random individuals from each of the other populations are selected and obtained solutions are evaluated. 
Next, each population is evolved in the same way as specified by a regular genetic algorithm.
For further evaluations, three attributes are considered when selecting collaborators 
\cite{stoean2014support}:
\begin{itemize}
	\item collaborator selection pressure, 
	\item collaboration pool size,
	\item collaboration credit assignment.
\end{itemize}
Collaborator selection pressure refers to the way of selecting collaborators from other populations when evaluating an individual. 
One can select the best individual according to its previous fitness score, pick a random individual or use classic selection schemes. 
Collaboration pool size represents the number of collaborators which are selected from each population. 
Collaboration credit assignment determines the way of computing the fitness of the current individual. This attribute is considered when the collaboration pool size is higher than 1. 
There are three possibilities for calculating the fitness value based on multiple collaborators:
\begin{itemize}
	\item optimistic: the fitness of the current individual is the value of its best collaboration,
	\item hedge: the fitness of the current individual is the average value of its collaborations,
	\item pessimistic: the fitness of the current individual is the value of its worst collaboration. 
\end{itemize}
An individual is evaluated by forming a number of collaborations according to the collaboration pool size. The collaborators are selected through a strategy corresponding to the collaboration selection pressure. 
The fitness value of an individual is measured as a result of its collaborations.
A way of assigning the fitness value to an individual based on its collaborations is defined by the collaboration credit assignment.
